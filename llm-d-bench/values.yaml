# llm-d-bench Helm Chart Values

# Namespace to deploy to
namespace: llm-d-inference-scheduling

# Benchmark tool to use (guidellm, or add more in the future)
benchmarkTool: guidellm

# Job type: "benchmark" or "cleanup"
jobType: benchmark

# Benchmark job configuration
benchmark:
  # Job name
  name: guidellm-test

  # Image
  image:
    repository: "image-registry.openshift-image-registry.svc:5000/llm-d-inference-scheduling/guidellm-runner"
    tag: "latest"

  # Environment variables for the benchmark container
  # Add any GUIDELLM__ prefixed vars here
  env:
    GUIDELLM__REQUEST_TIMEOUT: "1000"
    GUIDELLM__LOGGING__CONSOLE_LOG_LEVEL: "INFO"

  # GuideLLM benchmark parameters
  target: ""                    # Target URL (e.g., http://service:8080)
  model: ""                     # Model name (e.g., meta-llama/Llama-3.3-70B-Instruct)
  processor: ""                 # Processor/tokenizer model (optional, defaults to model if not set)
  backendType: "openai_http"    # Backend type (openai_http, openai_completions, etc.)
  rateType: "concurrent"        # Rate type (concurrent, synchronous, etc.)
  rate: ""                      # Rate values (e.g., "1,50,100,200")
  data: "1000"                  # Number of requests or data points
  maxSeconds: "600"             # Maximum seconds to run

  # Additional GuideLLM arguments (optional)
  # Use this for any flags not covered by the above parameters
  additionalArgs: ""

  # Node scheduling configuration (optional)
  # Ensure benchmark jobs do not run on GPU nodes
  # nodeSelector:
  #   node.kubernetes.io/instance-type: worker

  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: nvidia.com/gpu.present
  #           operator: DoesNotExist

# Cleanup job configuration
cleanup:
  # Cleanup job name
  name: cleanup

  # Node scheduling configuration (optional)
  # Ensure cleanup jobs do not run on GPU nodes
  # nodeSelector:
  #   node.kubernetes.io/instance-type: worker

  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: nvidia.com/gpu.present
  #           operator: DoesNotExist

# Log pod matching label for collecting logs from model serving pods
logPodMatchingLabel: "llm-d.ai/model=ms-inference-scheduling-llm-d-modelservice"

# Monitoring configuration
monitoring:
  # Enable metrics collection during benchmark
  # When enabled, deploys a dedicated monitoring sidecar container
  enabled: false

  # RBAC and infrastructure setup for monitoring
  # Set to false if your cluster already has the necessary RBAC, ServiceMonitor, etc.
  rbac:
    enabled: true

  # Monitoring sidecar container configuration
  sidecar:
    image:
      repository: "image-registry.openshift-image-registry.svc:5000/llm-d-inference-scheduling/vllm-metrics-collector"
      tag: "latest"

    # Resource allocation for monitoring sidecar
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "500m"

  # Thanos Querier endpoint for vLLM metrics collection
  thanosUrl: "https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"

  # Collection interval in seconds
  collectionInterval: 10

  # Specific vLLM metrics to collect (leave empty to use defaults)
  # Default metrics are listed below. You can override by providing a custom list.
  # Full list: https://docs.vllm.ai/en/latest/design/metrics.html
  metrics:
    - "vllm:num_requests_running"
    - "vllm:num_requests_waiting"
    - "vllm:num_requests_swapped"
    - "vllm:gpu_cache_usage_perc"
    - "vllm:cpu_cache_usage_perc"
    - "vllm:avg_prompt_throughput_toks_per_s"
    - "vllm:avg_generation_throughput_toks_per_s"
    - "vllm:time_to_first_token_seconds"
    - "vllm:time_per_output_token_seconds"
    - "vllm:e2e_request_latency_seconds"
    - "vllm:num_preemptions_total"
    - "vllm:prompt_tokens_total"
    - "vllm:generation_tokens_total"

  # Label filters for vLLM metrics (e.g., model=llama)
  # Format: key1=value1,key2=value2
  labels: ""

  # Collect node-level metrics (CPU, memory, network, disk I/O)
  # Set to false to only collect vLLM metrics
  collectNodeMetrics: true

  # Log level for monitoring script (DEBUG, INFO, WARNING, ERROR)
  logLevel: "INFO"

# PVC configuration
pvc:
  # Create the PVC (set to true if PVC doesn't exist yet)
  # Set to false to reuse an existing PVC across multiple experiments
  create: false

  # PVC name - use the same name across experiments to share results
  name: guidellm-pvc

  # PVC size (only applies if create: true)
  size: 50Gi

# Kueue batching configuration
kueue:
  # Enable Kueue batching
  enabled: false
  # Kueue queue name
  queueName: "guidellm-jobs"

# S3 configuration
s3:
  # Enable S3 upload
  enabled: false

  # S3 bucket name
  bucket: ""

  # S3 endpoint URL
  endpoint: ""

  # S3 region
  region: ""

  # Secret containing S3 credentials
  # The secret must have keys: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
  secretName: "aws-credentials"

