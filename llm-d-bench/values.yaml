# llm-d-bench Helm Chart Values

# Namespace to deploy to
namespace: llm-d-inference-scheduler

# Benchmark tool to use (guidellm, or add more in the future)
benchmarkTool: guidellm

# Benchmark job configuration
benchmark:
  # Job name
  name: guidellm-test

  # Image - For custom images, specify repository and tag.
  # If repository is empty, the default guidellm image with configmap script will be used.
  image:
    repository: ""
    tag: ""
    pullPolicy: IfNotPresent

  # Environment variables for the benchmark container
  # Add any GUIDELLM__ prefixed vars here
  env:
    GUIDELLM__REQUEST_TIMEOUT: "1000"
    GUIDELLM__LOGGING__CONSOLE_LOG_LEVEL: "INFO"

  # GuideLLM benchmark parameters
  target: ""                    # Target URL (e.g., http://service:8080)
  model: ""                     # Model name (e.g., meta-llama/Llama-3.3-70B-Instruct)
  processor: ""                 # Processor/tokenizer model (optional, defaults to model if not set)
  backendType: "openai_http"    # Backend type (openai_http, openai_completions, etc.)
  rateType: "concurrent"        # Rate type (concurrent, synchronous, etc.)
  rate: ""                      # Rate values (e.g., "1,50,100,200")
  data: "1000"                  # Number of requests or data points
  maxSeconds: "600"             # Maximum seconds to run
  accelerator: ""               # Accelerator type (e.g., nvidia-h100, nvidia-a100)

  # Additional GuideLLM arguments (optional)
  # Use this for any flags not covered by the above parameters
  additionalArgs: ""

  # Node scheduling configuration (optional)
  # Ensure benchmark jobs do not run on GPU nodes
  # nodeSelector:
  #   node.kubernetes.io/instance-type: worker

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.present
            operator: DoesNotExist

# Log pod matching label for collecting logs from model serving pods
logPodMatchingLabel: "llm-d.ai/model=ms-inference-scheduling-llm-d-modelservice"

# Kueue batching configuration
kueue:
  # Enable Kueue batching
  enabled: false
  # Kueue queue name
  queueName: "guidellm-jobs"

  # Multi-cluster configuration
  multiCluster:
    # Enable multi-cluster support
    enabled: false
    # Target cluster name (optional - if not specified, uses default cluster)
    # This should match a ClusterQueue name in your Kueue setup
    targetCluster: ""

# MLflow configuration
mlflow:
  # Enable MLflow experiment tracking
  enabled: false

  # MLflow tracking server URL
  # Example: "http://mlflow.mlflow.svc.cluster.local:5000" (internal service)
  # or "https://mlflow-mlflow.apps.your-cluster.com" (via route)
  trackingUri: ""

  # MLflow experiment name (if not provided, will use benchmark.name)
  experimentName: ""

  # MLflow authentication secret
  # NOTE: The secret must exist in the same namespace as the benchmark job.
  # To use the mlflow-auth secret from the mlflow namespace, copy it:
  #   oc get secret mlflow-auth -n mlflow -o yaml | \
  #   sed 's/namespace: mlflow/namespace: <your-namespace>/' | \
  #   oc apply -f -
  auth:
    # Secret name containing MLflow credentials (in the same namespace)
    secretName: "mlflow-auth"
    # Keys in the secret
    usernameKey: "admin-username"
    passwordKey: "admin-password"

  # S3 credentials for MLflow artifact store
  s3:
    # Secret name containing S3 credentials
    secretName: "mlflow-s3-creds"
    # Keys in the secret for S3 credentials
    accessKey: "AWS_ACCESS_KEY_ID"
    secretKey: "AWS_SECRET_ACCESS_KEY"
    bucketNameKey: "bucket-name"
    regionKey: "region"

