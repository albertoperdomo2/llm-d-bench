# llm-d-bench Helm Chart Values

# Namespace to deploy to
namespace: llm-d-inference-scheduler

# Benchmark tool to use (guidellm, or add more in the future)
benchmarkTool: guidellm

# Job type: "benchmark" or "cleanup"
jobType: benchmark

# Benchmark job configuration
benchmark:
  # Job name
  name: guidellm-test

  # Image
  image:
    repository: "image-registry.openshift-image-registry.svc:5000/llm-d-inference-scheduler/guidellm-runner"
    tag: "latest"

  # Environment variables for the benchmark container
  # Add any GUIDELLM__ prefixed vars here
  env:
    GUIDELLM__REQUEST_TIMEOUT: "1000"
    GUIDELLM__LOGGING__CONSOLE_LOG_LEVEL: "INFO"

  # GuideLLM benchmark parameters
  target: ""                    # Target URL (e.g., http://service:8080)
  model: ""                     # Model name (e.g., meta-llama/Llama-3.3-70B-Instruct)
  processor: ""                 # Processor/tokenizer model (optional, defaults to model if not set)
  backendType: "openai_http"    # Backend type (openai_http, openai_completions, etc.)
  rateType: "concurrent"        # Rate type (concurrent, synchronous, etc.)
  rate: ""                      # Rate values (e.g., "1,50,100,200")
  data: "1000"                  # Number of requests or data points
  maxSeconds: "600"             # Maximum seconds to run

  # Additional GuideLLM arguments (optional)
  # Use this for any flags not covered by the above parameters
  additionalArgs: ""

  # Node scheduling configuration (optional)
  # Ensure benchmark jobs do not run on GPU nodes
  # nodeSelector:
  #   node.kubernetes.io/instance-type: worker

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.present
            operator: DoesNotExist

# Cleanup job configuration
cleanup:
  # Cleanup job name
  name: cleanup

  # Node scheduling configuration (optional)
  # Ensure cleanup jobs do not run on GPU nodes
  # nodeSelector:
  #   node.kubernetes.io/instance-type: worker

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.present
            operator: DoesNotExist

# Log pod matching label for collecting logs from model serving pods
logPodMatchingLabel: "llm-d.ai/model=ms-inference-scheduling-llm-d-modelservice"

# PVC configuration
pvc:
  # Create the PVC (set to true if PVC doesn't exist yet)
  # Set to false to reuse an existing PVC across multiple experiments
  create: false

  # PVC name - use the same name across experiments to share results
  name: guidellm-pvc

  # PVC size (only applies if create: true)
  size: 50Gi
