# Qwen 0.6B Baseline Benchmark Experiment
# Tests basic performance with simple token specifications

namespace: llm-d-inference-scheduling

benchmark:
  name: qwen-0.6b-kueue

  image:
    repository: "ghcr.io/vllm-project/guidellm"
    tag: "latest"

  affinity: null

  # Target inference endpoint
  target: http://infra-inference-scheduling-inference-gateway-istio.llm-d-inference-scheduling.svc.cluster.local:80

  # Model configuration
  model: Qwen/Qwen3-0.6B
  processor: Qwen/Qwen3-0.6B

  # Benchmark parameters
  rateType: concurrent
  rate: "1,50,100,200,300,500,650"
  data: "prompt_tokens=1000,output_tokens=1000"
  maxSeconds: "600"

  # Environment variables (optional)
  env:
    GUIDELLM__REQUEST_TIMEOUT: "1000"
    GUIDELLM__LOGGING__CONSOLE_LOG_LEVEL: "INFO"

# PVC configuration
# Set create: true only for the first experiment run, then set to false to reuse
pvc:
  create: false
  name: guidellm-pvc
  size: "20Gi"

# Kueue configuration
kueue:
  enabled: true
  queueName: "guidellm-jobs"

# S3 configuration
s3:
  enabled: true
  bucket: ""
  endpoint: ""
  region: "us-east-1"
  secretName: "aws-credentials"
