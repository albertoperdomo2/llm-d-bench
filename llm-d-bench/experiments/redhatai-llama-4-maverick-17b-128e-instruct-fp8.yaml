# RedHatAI Llama-3.3-70B Instruct FP8 dynamic Baseline Benchmark Experiment
# Tests basic performance with simple token specifications

benchmark:
  name: redhatai-llama-4-maverick-17b-128e-instruct-fp8

  # Target inference endpoint
  target: http://infra-inference-scheduling-inference-gateway-istio.llm-d-inference-scheduler.svc.cluster.local:80

  # Model configuration
  model: RedHatAI/Llama-4-Maverick-17B-128E-Instruct-FP8
  processor: RedHatAI/Llama-4-Maverick-17B-128E-Instruct-FP8

  # Benchmark parameters
  rateType: concurrent
  rate: "1,50,100,200,300,500,650"
  data: "prompt_tokens=1000,output_tokens=1000"
  maxSeconds: "600"

  # Environment variables (optional)
  env:
    GUIDELLM__REQUEST_TIMEOUT: "1000"
    GUIDELLM__LOGGING__CONSOLE_LOG_LEVEL: "INFO"

# Log pod matching label for collecting logs from model serving pods
logPodMatchingLabel: "llm-d.ai/model=ms-inference-scheduling-llm-d-modelservice"

# PVC configuration
# Set create: true only for the first experiment run, then set to false to reuse
pvc:
  create: false
  name: guidellm-pvc
  size: "20Gi"

# Kueue configuration
kueue:
  enabled: true
  queueName: "guidellm-jobs"

# S3 configuration
s3:
  enabled: true
  bucket: ""
  endpoint: ""
  region: "us-east-1"
  secretName: "aws-credentials"
